{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "djdata = list(csv.DictReader(open('DJI From 2004.csv')))\n",
    "vixdata = list(csv.DictReader(open('vixcurrent.csv')))\n",
    "\n",
    "djchange = [row['Change'] for row in djdata]\n",
    "vixchange = [row['Change'] for row in vixdata]\n",
    "\n",
    "djch = [float(i) for i in djchange]\n",
    "vixch = [float(i) for i in vixchange]\n",
    "\n",
    "\n",
    "print(len(djch))\n",
    "print(len(vixch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "####\n",
    "#\n",
    "# LINEAR ALGEBRA\n",
    "#\n",
    "####\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "####\n",
    "#\n",
    "# STATS\n",
    "#\n",
    "####\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"assumes x has at least two elements\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "####\n",
    "#\n",
    "# CORRELATION\n",
    "#\n",
    "#####\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n - 1)\n",
    "\n",
    "def correlation(x, y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x, y) / (stdev_x * stdev_y)\n",
    "    else:\n",
    "        return 0 # if no variation, correlation is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('\\nScatter plot for daily minutes vs number of friends:')\n",
    "_ = plt.scatter(vixch, djch)\n",
    "_ = plt.xlim(-0.5,0.5)\n",
    "_ = plt.ylim(-0.5,0.5)\n",
    "_ = plt.xlabel('The change of Vix')\n",
    "_ = plt.ylabel('The change of DJ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Correlation before outlier removal: {:.3f}'.format(correlation(djch, vixch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    \"\"\"the linear model\n",
    "    alpha and beta are our model parameters\n",
    "    x_i is the data point\n",
    "    return the predicted y value for x_i\"\"\"\n",
    "    return beta * x_i + alpha\n",
    "\n",
    "def error(alpha, beta, x_i, y_i):\n",
    "    \"\"\"the difference between the true y_i\n",
    "    and our predicted y value\"\"\"\n",
    "    return y_i - predict(alpha, beta, x_i)\n",
    "\n",
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "def least_squares_fit(x,y):\n",
    "    \"\"\"given training values for x and y,\n",
    "    find the least-squares values of alpha and beta\"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta\n",
    "\n",
    "def total_sum_of_squares(y):\n",
    "    \"\"\"the total squared variation of y_i's from their mean\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"the fraction of variation in y captured by the model, which equals\n",
    "    1 - the fraction of variation in y not captured by the model\"\"\"\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) /\n",
    "                  total_sum_of_squares(y))\n",
    "\n",
    "alpha, beta = least_squares_fit(vixch, djch)\n",
    "print(\"alpha\", alpha)\n",
    "print(\"beta\", beta)\n",
    "\n",
    "r_squared_value = r_squared(alpha, beta, vixch, djch)\n",
    "print(\"r-squared\", r_squared_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error(alpha, beta, x, y):\n",
    "    return math.sqrt(sum_of_squared_errors(alpha, beta, x, y) / len(y))\n",
    "\n",
    "stderr = standard_error(alpha, beta, vixch, djch)\n",
    "print(\"standard error\", stderr)\n",
    "\n",
    "\n",
    "print('\\nScatter plot with regression line and 95% prediction interval:')\n",
    "_ = plt.scatter(vixch, djch)\n",
    "_ = plt.xlim(-0.5,0.5)\n",
    "_ = plt.ylim(-0.2,0.2)\n",
    "_ = plt.xlabel('The change of Vix')\n",
    "_ = plt.ylabel('The change of DJ')\n",
    "_ = plt.plot([-10,10], [predict(alpha,beta,-10), predict(alpha,beta,10)], 'r',\n",
    "             [-10,10], [predict(alpha,beta,-10)-2*stderr, predict(alpha,beta,10)-2*stderr], 'r--',\n",
    "             [-10,10], [predict(alpha,beta,-10)+2*stderr, predict(alpha,beta,10)+2*stderr], 'r--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 - \n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, slope_stderr = linregress(vixch,djch)\n",
    "\n",
    "print('\\nComparing linear regression values:')\n",
    "print('-'*47)\n",
    "print('{:15} {:>15} {:>15}'.format('Statistic', 'From Scipy', 'From Scratch'))\n",
    "print('-'*47)\n",
    "for name, sc_val, sp_val in [(\"alpha\", intercept, alpha),\n",
    "                             (\"beta\", slope, beta),\n",
    "                             (\"r-squared\", r_squared_value, r_value**2)]:\n",
    "    print('{:15} {:15.2f} {:15.2f}'.format(name, sc_val, sp_val))\n",
    "print('-'*47)\n",
    "\n",
    "# 2 - \n",
    "print('\\np-value for a two-sided test of H0 that slope is 0: {}'.format(p_value))\n",
    "print('Can we reject H0?', 'Yes' if p_value<=0.01 else 'No')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrv = np.array(vixch)\n",
    "arrd = np.array(djch)\n",
    "\n",
    "vixchr = arrv.reshape(-1,1)\n",
    "djchr = arrd.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# First let's create a train and test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vixchr, djchr, test_size=0.33,\n",
    "                                                    random_state=2) # so we get the same results\n",
    "\n",
    "# Now let's fit a model\n",
    "lm = LinearRegression()\n",
    "_ = lm.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print('Intercept:', lm.intercept_)\n",
    "print('Coefficients:\\n', lm.coef_)\n",
    "print('\\nR-squared:', lm.score(X_train, Y_train))\n",
    "\n",
    "# We use the score method to get r-squared\n",
    "print('\\nR-squared:', lm.score(X_train, Y_train))\n",
    "\n",
    "# We can predict the median price for a new neighbourhoods\n",
    "print('\\nPredicted price of first five neighbourhoods from test split:\\n', lm.predict(X_test)[:5])\n",
    "\n",
    "# We use the score method to get r-squared\n",
    "print('\\nR-squared:', lm.score(X_train, Y_train))\n",
    "\n",
    "# We can also calculate the standard error\n",
    "stderr = math.sqrt(np.mean((Y_train - lm.predict(X_train))**2))\n",
    "print('\\nStandard error:', stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "X = vixchr\n",
    "y = djchr\n",
    "\n",
    "est = sm.OLS(y, X)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Yes, r_squared indicates that our model explains the data reasonable well.\n",
    "#     But we should look at standard error as well (page 183 of Data Science from Scratch).\n",
    "\n",
    "# 2 - \n",
    "from sklearn.metrics import mean_squared_error\n",
    "se1 = math.sqrt(np.mean((Y_train - lm.predict(X_train))**2))\n",
    "print('Standard error:', se1)\n",
    "se2 = math.sqrt(mean_squared_error(Y_train, lm.predict(X_train)))\n",
    "print('Standard error using sklearn.metrics:', se2)\n",
    "\n",
    "# 3 - Note we have a fairly small data set (339 in training, 167 in test).\n",
    "#     So this value will vary depending on our split. \n",
    "print('\\nPredictions should be within {:.1f}k dollars at 95% confidence according to training data.'.format(2*se2))\n",
    "se_test = math.sqrt(mean_squared_error(Y_test, lm.predict(X_test)))\n",
    "print('Predictions should be within {:.1f}k dollars at 95% confidence according to test data.'.format(2*se_test))\n",
    "\n",
    "# 4 - Yes the linear model is appropriate\n",
    "print('\\nResidual plot for training data (blue) and test data (green):')\n",
    "_ = plt.scatter(lm.predict(X_train), Y_train-lm.predict(X_train), c='blue', s=40, alpha=0.5, edgecolor='white')\n",
    "_ = plt.scatter(lm.predict(X_test), Y_test-lm.predict(X_test), c='green', s=40, alpha=0.5, edgecolor='white')\n",
    "_ = plt.plot([-0.1,0.1], [0,0], c='black')\n",
    "_ = plt.ylabel('Residuals ($y - \\hat{y}$)')\n",
    "_ = plt.xlabel('Predicted values ($\\hat{y}$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "djdata = list(csv.DictReader(open('DJI From 2004v.csv')))\n",
    "vixdata = list(csv.DictReader(open('vixcurrent.csv')))\n",
    "\n",
    "djch = [row['trend'] for row in djdata]\n",
    "djvlm = [row['Volume'] for row in djdata]\n",
    "vixchange = [row['vixchgystd'] for row in djdata]\n",
    "vixcha = [float(i) for i in vixchange]\n",
    "\n",
    "a=list(itertools.chain.from_iterable(zip(vixcha,djvlm)))\n",
    "arr = np.array(a)\n",
    "vixvlm= arr.reshape(-1,2)\n",
    "pprint.pprint((vixvlm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import pprint\n",
    "iris = load_iris()\n",
    "key='-1=down, 0 = horizontal ,1=up '\n",
    "#pprint.pprint(key)\n",
    "#pprint.pprint(iris.data)\n",
    "#pprint.pprint(iris.target)\n",
    "#print(len(iris.target))\n",
    "#print(len(iris.data))\n",
    "\n",
    "# First let's create a train and test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vixvlm, djch, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Let's fit a model\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "_ = tree.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate\n",
    "print('Classification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html\n",
    "def get_code(tree, feature_names, target_names, spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            print(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")\n",
    "            if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "            print(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "            print(spacer + \"}\")\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],\n",
    "                            target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                print(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)\n",
    "    \n",
    "print('Decision tree:\\n')\n",
    "get_code(tree, ['vixchange','volume'], ['-1','0','1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "export_graphviz(tree,out_file='tree.dot',\n",
    "                feature_names = ['vixchange','volume'],\n",
    "                class_names =['-1','0','1'],\n",
    "                rounded = True, proportion =False,\n",
    "                precision = 2, filled = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def mcnemar(x, y):\n",
    "    n1 = np.sum(x < y)\n",
    "    n2 = np.sum(x > y)\n",
    "    stat = (np.abs(n1-n2)-1)**2 / (n1+n2)\n",
    "    df = 1\n",
    "    pval = chi2.sf(stat,1)\n",
    "    return stat, pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Perform grid search\n",
    "param_grid = [\n",
    "    {'max_depth': [2, 3, 4, 5, 6],\n",
    "     'criterion': ['entropy', 'gini'],\n",
    "     'splitter': ['best', 'random']}\n",
    "]\n",
    "tree = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "# Print grid search results\n",
    "print('Grid search mean and stdev:\\n')\n",
    "scoring = tree.cv_results_\n",
    "for mean_score, std, params in zip(scoring['mean_test_score'],scoring['std_test_score'],scoring['params']):\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, std * 2, params))\n",
    "    \n",
    "#for params, mean_score, scores in tree.cv_results_:\n",
    "   # print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "       #     mean_score, scores.std() * 2, params))\n",
    "\n",
    "# Print best params\n",
    "print('\\nBest parameters:', tree.best_params_)\n",
    "\n",
    "# Evaluate on held-out test\n",
    "print('\\nClassification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
